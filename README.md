# Spaceship-Titanic

This was for a practice ML competition for Kaggle, in which one is given a dataset of approximately 8700 passengers aboard a ship in which about half of them are transported to another dimension.  The columns include a lot of categorical data, such as **HomePlanet**, **Destination**(Planet), a **CryoSleep** flag, **Age**, expenditures in various commercial districts aboard the ship, etc.  This is a supervised ML challenge, so it also includes a Boolean **Transported** column.  Separately, there is a dataset of about 4300 passengers in which the **Transported** column is absent.  This is the crux of the competition--as Kaggle has that column to judge model performance of the participants.

Here I end up selecting a Logistic Regression model from sklearn, which yields about 79% accuracy on the test set.  I essentially just use numerical encoders for the categorical features, but I do convert **Cabin** into a tuple of components first (in such a way that the middle number does get broken though, i.e. 'A/100/S' --> ('A','1','0','0','S') ) prior to encoding.

I also consider two cases: one where I remove all rows with null values for training, and one where I do not.  These two cell branches are commented: *cell number/total cells*.  Also note, when I concatenate all of the numpy arrays for **x_train**, I always throw the **Cabin** data at the end, but the rest of the columns are kept in order.  In the case where rows with null entries are not removed (including with the test set), an imputer is used for the numerical columns (i.e. **Age** and the spending columns).
